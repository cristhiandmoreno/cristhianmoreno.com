---
title: "Escribir para la máquina: detectores de IA e indefensión
estudiantil"
date: 2025-12-18
description: "El problema de la evaluación de la IA en textos académicos"
author: kevin
opinion: true # Si está en false, porque no es una opinión, hay que agregar el tipo de artículo "@type": "OpinionArticle", "Article", "NewsArticle"
discusion: true 
categories: ["Opinión"]
tags: ["Tecnologia", "Inteligencia Artificial"]
image: /assets/img/publicaciones/el-inconforme/verificacion-ia.jpeg
alt: "IA"
---

Los detectores de inteligencia artificial surgieron con un propósito que es legítimo: prevenir
el plagio y evaluar las capacidades reales de las personas en contextos académicos y
profesionales, evitando que haya una dependencia acrítica de herramientas automatizadas.
Reconozco este objetivo porque el problema no está en la existencia de estas tecnologías,
sino en la forma en la que están siendo utilizadas. En particular, su adopción como
instrumentos o jueces punitivos dentro de los sistemas de evaluación académica, ha abierto
una serie de vacíos que afectan directamente al estudiantado y que merecen que hagamos una
reflexión crítica.


El primer vacío por mencionar es la opacidad. En cualquier procedimiento sancionatorio para
usar y acceder al derecho a la defensa se necesita de conocer con claridad la imputación y los
fundamentos que la sostienen. Ahora bien, cuando la sospecha de uso de IA se basa en un
índice probabilístico generado por un modelo que no puede ser explicado de manera
determinista, ese derecho se vuelve casi ilusorio. No se le acusa al evaluado sobre una acción
concreta verificable, sino una probabilidad estadística. El estudiante no sabe exactamente qué
se le reprocha ni qué criterios específicos condujeron a esa conclusión, y por tanto tampoco
sabe cómo rebatirla.


Los detectores de IA han sido implementados y protocolizados, sí. Pero en una ruta
institucional marcadamente punitiva. Las universidades suelen establecer porcentajes de
tolerancia y sanciones automáticas ya sea la disminución de la nota, anulación del trabajo o
incluso consecuencias disciplinarias más graves. No obstante, rara vez se diseñan
mecanismos claros, eficaces y transparentes para impugnar una detección errónea. La
decisión ya no la toma el profesor en ejercicio de su juicio pedagógico, sino un programa
contratado por la institución (hipócrita o irónicamente son otras IA). La pregunta por la
responsabilidad queda entonces suspendida; cuando el sistema falla, ni el software ni la
universidad parecen asumir plenamente las consecuencias, dejando al estudiante en una
situación de indefensión y en una permanente incertidumbre ansiosa.


Este modelo no solo es problemático desde un punto de vista procedimental, sino también
desde el pedagógico. Muchos detectores de IA afirman basarse en rasgos como coherencia,
fluidez, claridad sintáctica o precisión conceptual; es decir, cualidades que históricamente han
sido promovidas por la universidad como signos de buena escritura académica. El resultado
es paradójico terminando con el estudiante que comienza a escribir no para argumentar mejor,
sino para no parecer demasiado correcto, para introducir torpezas estratégicas o estilos
artificialmente desprolijos que reduzcan el riesgo de ser señalado por un algoritmo. La
escritura deja de orientarse al lector humano y se dirige, por temor, a un lector artificial.
En este punto, vemos al problema trascender lo técnico y revelarnos una dimensión más
profunda: el surgir de una nueva forma de autoridad. Una autoridad que no razona, no dialoga
ni explica, pero que produce efectos reales sobre las trayectorias académicas. El juicio
pedagógico es desplazado por un dictamen probabilístico que se presenta como neutral y
objetivo, aun cuando sus márgenes de error y sus criterios internos no sean plenamente
conocidos. Se trata de una autoridad que vigila sin mostrarse y sanciona sin dar razones
completas.


Aquí me permito recordar el concepto de panoptismo desarrollado por Michel Foucault.
Como en el panóptico, el poder no necesita observar todo el tiempo para ser eficaz basta con
que el sujeto interiorice la posibilidad de ser observado. Algo similar ocurre cuando el
estudiante escribe bajo la sospecha constante de la detección algorítmica de las IAs que use la
universidad. No sabe cuándo ni cómo será evaluado por la máquina, pero ajusta su conducta
en función de esa mirada invisible. La consecuencia es una forma de autocensura que
empobrece el proceso educativo.


El uso acrítico de detectores de inteligencia artificial en la evaluación académica no solo nos
plantea dudas sobre su fiabilidad técnica, sino que compromete principios básicos del debido
proceso, altera los fines pedagógicos de la escritura y consolida una autoridad que no asume
responsabilidad por sus errores. Mientras estas herramientas no sean transparentes,
discutibles y subordinadas al juicio humano, su aplicación punitiva corre el riesgo de
erosionar aquello que la universidad dice defender, el pensamiento crítico, la formación
integral y la confianza en la razón argumentada.